{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# robot parser\n",
    "Robots协议也称作爬虫协议、机器人协议，它的全名叫作网络爬虫排除标准（Robots Exclusion Protocol），用来告诉爬虫和搜索引擎哪些页面可以抓取，哪些不可以抓取。它通常是一个叫作robots.txt的文本文件，一般放在网站的根目录下。\n",
    "```\n",
    "User-agent: *\n",
    "Disallow: /\n",
    "Allow: /public/\n",
    "```\n",
    "上面的User-agent描述了搜索爬虫的名称，这里将其设置为*则代表该协议对任何爬取爬虫有效。比如，我们可以设置：\n",
    "```\n",
    "User-agent: Baiduspider # 百度爬虫的名称\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['# See http://www.robotstxt.org/wc/norobots.html for documentation on how to use the robots.txt file',\n '#',\n '# To ban all spiders from the entire site uncomment the next two lines:',\n 'User-agent: *',\n 'Disallow: /search',\n 'Disallow: /convos/',\n 'Disallow: /notes/',\n 'Disallow: /admin/',\n 'Disallow: /adm/',\n 'Disallow: /p/0826cf4692f9',\n 'Disallow: /p/d8b31d20a867',\n 'Disallow: /collections/*/recommended_authors',\n 'Disallow: /trial/*',\n 'Disallow: /keyword_notes',\n 'Disallow: /stats-2017/*',\n '',\n 'User-agent: trendkite-akashic-crawler',\n 'Request-rate: 1/2 # load 1 page per 2 seconds',\n 'Crawl-delay: 60',\n '',\n 'User-agent: YisouSpider',\n 'Request-rate: 1/10 # load 1 page per 10 seconds',\n 'Crawl-delay: 60',\n '',\n 'User-agent: Cliqzbot',\n 'Disallow: /',\n '',\n 'User-agent: Googlebot',\n 'Request-rate: 2/1 # load 2 page per 1 seconds',\n 'Crawl-delay: 10',\n 'Allow: /',\n '',\n 'User-agent: Mediapartners-Google',\n 'Allow: /',\n '']"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "import urllib.request\n",
    "header = headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\"}\n",
    "url = 'https://www.jianshu.com/robots.txt'\n",
    "request = urllib.request.Request(url, headers=header)\n",
    "res = urllib.request.urlopen(request)\n",
    "res.read().decode('utf-8').split('\\n')\n",
    "# 似乎现在看起来都用的是https协议，直接用urlopen好像无法打开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "False\nFalse\n"
    }
   ],
   "source": [
    "from urllib.robotparser import RobotFileParser # 载入RobotFileParser类\n",
    "\n",
    "rp = RobotFileParser() # 创建robot parser实例\n",
    "# rpa = urllib.robotparser.RobotFileParser(url='/link/to/robots.txt')\n",
    "\n",
    "rp.set_url('https://www.jianshu.com/robots.txt') # 设置robots.txt的路径\n",
    "rp.read() # read the robots.txt file \n",
    "\n",
    "print(rp.can_fetch('Googlebot', 'http://www.jianshu.com/p/b67554025d7d'))\n",
    "'''\n",
    "can_fetch()：该方法传入两个参数，第一个是User-agent，第二个是要抓取的URL。返回的内容是该搜索引擎是否可以抓取这个URL，返回结果是True或False。\n",
    "'''\n",
    "print(rp.can_fetch('*', \"http://www.jianshu.com/search?q=python&page=1&type=collections\"))\n",
    "\n",
    "## 因为现在都有反爬虫策略，所以直接打开应该不行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "True\nTrue\n"
    }
   ],
   "source": [
    "# 使用parser解析\n",
    "header = headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11\"}\n",
    "url = 'https://www.jianshu.com/robots.txt'\n",
    "request = urllib.request.Request(url, headers=header)\n",
    "res = urllib.request.urlopen(request)\n",
    "rp = RobotFileParser()\n",
    "rp.parse(res.read().decode('utf-8').split('\\n'))\n",
    "print(rp.can_fetch('Googlebot', 'http://www.jianshu.com/p/b67554025d7d'))\n",
    "print(rp.can_fetch('Googlebot', \"http://www.jianshu.com/search?q=python&page=1&type=collections\"))"
   ]
  }
 ]
}